apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "vector-copilot.configMapName" . }}
  labels:
    {{- include "vector-copilot.labels" . | nindent 4 }}
data:
  vector.yaml: |
    # Vector configuration for syslog TLS ingestion and Azure DCR forwarding
    
    # Data directory for Vector state and disk buffers
    data_dir: /var/lib/vector
    
    {{- if .Values.vectorApi.enabled }}
    # Vector API for health checks and metrics
    api:
      enabled: true
      address: {{ .Values.vectorApi.address | quote }}
      playground: false
    {{- end }}
    
    # Source: Syslog over TLS on port 6514
    sources:
      syslog_tls:
        type: socket
        address: "0.0.0.0:6514"
        mode: tcp
        max_length: 102400
        # TLS configuration
        tls:
          enabled: true
          # Certificate and key mounted from Kubernetes secret
          crt_file: /etc/vector/tls/tls.crt
          key_file: /etc/vector/tls/tls.key
          # Optional: require client certificates for mutual TLS
          # ca_file: /etc/vector/tls/ca.crt
          # verify_certificate: true
    
    # Transform: Parse syslog and add metadata
    transforms:
      parse_syslog:
        type: remap
        inputs:
          - syslog_tls
        source: |
          # Parse syslog message
          . = parse_syslog!(.message)
          
          # Add metadata about ingestion port
          .meta.port_received = 6514
          
          # Trim whitespace from message for CEF detection
          .message = strip_whitespace!(.message)
      
      # Filter: Only forward CEF events
      filter_cef:
        type: filter
        inputs:
          - parse_syslog
        condition:
          type: vrl
          source: |
            # Check if message starts with "CEF:"
            starts_with(.message, "CEF:")
      
      # Route: Compute hash bucket for load distribution
      compute_hash:
        type: remap
        inputs:
          - filter_cef
        source: |
          # Build stable hash key from configured fields
          {{- $fields := .Values.hashSplit.keyFields }}
          hash_key = ""
          {{- range $fields }}
          hash_key = hash_key + string!({{ . }})
          {{- end }}
          
          # Compute CRC32 hash
          hash_value = crc32(hash_key)
          
          # Determine bucket (0 or 1 for 50/50 split)
          .route.hash_bucket = hash_value % {{ .Values.hashSplit.buckets }}
      
      # Route: Split to bucket 0 (DCR 1)
      route_dcr1:
        type: filter
        inputs:
          - compute_hash
        condition:
          type: vrl
          source: |
            .route.hash_bucket == 0
      
      # Route: Split to bucket 1 (DCR 2)
      route_dcr2:
        type: filter
        inputs:
          - compute_hash
        condition:
          type: vrl
          source: |
            .route.hash_bucket == 1
    
    # Sinks: Forward to Azure Data Collection Rules
    sinks:
      # Sink 1: Azure DCR endpoint 1 (bucket 0)
      azure_dcr1:
        type: http
        inputs:
          - route_dcr1
        uri: {{ .Values.dcr.dcr1.uri | quote }}
        method: post
        
        # Authentication: Bearer token from environment variable
        auth:
          strategy: bearer
          token: "${INGEST_TOKEN}"
        
        # Compression (recommended for Azure)
        compression: {{ .Values.http.compression | quote }}
        
        # Encoding: JSON array format
        # NOTE: Azure Log Ingestion expects JSON array format.
        # If single-document format is required, change to 'json' encoding
        # and adjust Content-Type header accordingly.
        encoding:
          codec: json
        
        # Headers required by Azure
        headers:
          Content-Type: "application/json"
        
        # Batch configuration
        batch:
          max_bytes: {{ .Values.http.batchMaxBytes }}
          timeout_secs: {{ .Values.http.batchTimeoutSeconds }}
        
        # Request configuration
        request:
          concurrency: {{ .Values.http.concurrency }}
          timeout_secs: {{ .Values.http.timeoutSeconds }}
          retry_attempts: 5
          retry_initial_backoff_secs: 1
          retry_max_duration_secs: 300
        
        # Disk buffer for reliability
        buffer:
          type: {{ .Values.buffer.type | quote }}
          {{- if eq .Values.buffer.type "disk" }}
          max_size: {{ .Values.buffer.maxSizeBytes }}
          when_full: {{ .Values.buffer.whenFull | quote }}
          {{- end }}
      
      # Sink 2: Azure DCR endpoint 2 (bucket 1)
      azure_dcr2:
        type: http
        inputs:
          - route_dcr2
        uri: {{ .Values.dcr.dcr2.uri | quote }}
        method: post
        
        # Authentication: Bearer token from environment variable
        auth:
          strategy: bearer
          token: "${INGEST_TOKEN}"
        
        # Compression (recommended for Azure)
        compression: {{ .Values.http.compression | quote }}
        
        # Encoding: JSON array format
        encoding:
          codec: json
        
        # Headers required by Azure
        headers:
          Content-Type: "application/json"
        
        # Batch configuration
        batch:
          max_bytes: {{ .Values.http.batchMaxBytes }}
          timeout_secs: {{ .Values.http.batchTimeoutSeconds }}
        
        # Request configuration
        request:
          concurrency: {{ .Values.http.concurrency }}
          timeout_secs: {{ .Values.http.timeoutSeconds }}
          retry_attempts: 5
          retry_initial_backoff_secs: 1
          retry_max_duration_secs: 300
        
        # Disk buffer for reliability
        buffer:
          type: {{ .Values.buffer.type | quote }}
          {{- if eq .Values.buffer.type "disk" }}
          max_size: {{ .Values.buffer.maxSizeBytes }}
          when_full: {{ .Values.buffer.whenFull | quote }}
          {{- end }}
